{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#importing required libraries.....\n",
    "\n",
    "import os \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re # regular expression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import nltk\n",
    "from nltk import PorterStemmer # natural language toolkit\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "# Tweet tokenizer does not split at apostophes which is what we want\n",
    "from nltk.tokenize import TweetTokenizer   \n",
    "lem = WordNetLemmatizer()\n",
    "tokenizer=TweetTokenizer()\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c2010d2d8ccb2a67715b8a9a0fd6d93ba04160b5"
   },
   "outputs": [],
   "source": [
    "os.listdir(\"input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9e9b6bfe9fb8a575952e19a8c17b0d064598847f"
   },
   "outputs": [],
   "source": [
    "#read the data \n",
    "train=pd.read_csv('input/train.csv')\n",
    "test=pd.read_csv('input/test.csv')\n",
    "sample=pd.read_csv('input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2f2759d607dacac36b5b0de0c41f94f15763eaac"
   },
   "outputs": [],
   "source": [
    "#printing some upper rows of our training data\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6e6072ace4cf591cd5f25fb4bd54f85366f18d1d"
   },
   "outputs": [],
   "source": [
    "#some information about data\n",
    "print('No. of training examples : ',len(train))\n",
    "print(\"No. of test data : \",len(test))\n",
    "print(train.columns[2:]) #columns_name\n",
    "row=train.iloc[:,2:].sum(axis=1)\n",
    "print(\"No. of examples with no labels : \",(row==0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ca1be5aa5bf35c26c6a602b87f60f75e11c4d2ab"
   },
   "outputs": [],
   "source": [
    "#fill blank values with unknown otherwise model gives problem\n",
    "print(\"Check for missing values in Train dataset\")\n",
    "null_check=train.isnull().sum()\n",
    "print(null_check)\n",
    "print(\"Check for missing values in Test dataset\")\n",
    "null_check=test.isnull().sum()\n",
    "print(null_check)\n",
    "print(\"filling NA with \\\"unknown\\\"\")\n",
    "train[\"comment_text\"].fillna(\"unknown\", inplace=True)\n",
    "test[\"comment_text\"].fillna(\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cc8cc99c2ee2bcd662248663faac48a633a68676",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot\n",
    "x=train.iloc[:,2:].sum()\n",
    "plt.figure(figsize=(8,4))\n",
    "ax= seaborn.barplot(x.index, x.values, alpha=0.8)\n",
    "plt.title(\"# per class\")\n",
    "plt.ylabel('# of Occurrences', fontsize=12)\n",
    "plt.xlabel('Type ', fontsize=12)\n",
    "#adding the text labels\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dd3f4988cb257c6314df024a2a3024f9b84d5baf"
   },
   "outputs": [],
   "source": [
    "# creating train-validation split\n",
    "x_train, x_val, y_train, y_val = train_test_split(train.comment_text, train.iloc[:,2:8], test_size=0.3, random_state=19)\n",
    "x_test = test.comment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5561dffcda357ddc8300a29c88e03a7872129308"
   },
   "outputs": [],
   "source": [
    "def clean(comment):\n",
    "    \"\"\"\n",
    "    This function receives comments and returns clean word-list\n",
    "    \"\"\"\n",
    "    #Convert to lower case , so that Hi and hi are the same\n",
    "    comment=comment.lower()\n",
    "    #remove \\n\n",
    "    comment=re.sub(\"\\\\n\",\"\",comment)\n",
    "    # remove leaky elements like ip,user\n",
    "    comment=re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\"\",comment)\n",
    "    #removing usernames\n",
    "    comment=re.sub(\"\\[\\[.*\\]\",\"\",comment)\n",
    "    \n",
    "    #Split the sentences into words\n",
    "    words=tokenizer.tokenize(comment)\n",
    "    \n",
    "    words = [w for w in words if not w in stopwords.words('english')]\n",
    "    words=[lem.lemmatize(word, \"v\") for word in words]\n",
    "    \n",
    "    clean_sent=\" \".join(words)\n",
    "    return(clean_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b4adf293ff9a256e65631d8778cdd504ce1861e8"
   },
   "outputs": [],
   "source": [
    "# preparing training text to pass in count vectorizer\n",
    "corpus=[]\n",
    "for text in x_train:\n",
    "    text = clean(text)\n",
    "    corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "944db1ae7e863069338486a96bfb2c2552dc56cd"
   },
   "outputs": [],
   "source": [
    "# build Count Vectorizer, to convert a collection of text documents to a matrix of token counts\n",
    "count_vect = CountVectorizer(ngram_range=(1,2))\n",
    "X_train_counts = count_vect.fit_transform(corpus)\n",
    "\n",
    "# build TFIDF Transformer, to transform a count matrix to a normalized tf or tf-idf representation\n",
    "# tfidf - term frequency inverse document frequency\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5dff76cfacd192362086c8c043df4c185ee4c34e"
   },
   "outputs": [],
   "source": [
    "# preparing validation text to pass in count vectorizer\n",
    "X_val_set = []\n",
    "for text in x_val:\n",
    "    text = clean(text)\n",
    "    X_val_set.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f51881d5733e6fd50a61d7c9f9258408222346ff"
   },
   "outputs": [],
   "source": [
    "# tranforming validation data using count vectorizer followed by tfidf transformer\n",
    "X_val_counts = count_vect.transform(X_val_set)\n",
    "X_val_tfidf = tfidf_transformer.transform(X_val_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "00feb9cdc2a5e9c67a4cc894f041e76a97f8a47c"
   },
   "outputs": [],
   "source": [
    "# preparing test text to pass in count vectorizer\n",
    "X_test_set = []\n",
    "for text in x_test:\n",
    "    text=clean(text)\n",
    "    X_test_set.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b91897ed88926e6feeac735e5329c9f916812244"
   },
   "outputs": [],
   "source": [
    "# tranforming validation data using count vectorizer followed by tfidf transformer\n",
    "X_test_counts = count_vect.transform(X_test_set)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "302acc808a042e7fb8bd6524b53564593c2c5279"
   },
   "outputs": [],
   "source": [
    "# creating dictionary to store prediction results\n",
    "result_test = dict()\n",
    "result_val =  dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9da7add340b0f5219c095b731f6b02c1de62b2d3"
   },
   "outputs": [],
   "source": [
    "#Applying Model\n",
    "# Multinomial Naive Bayes Model\n",
    "MNB_classifier = OneVsRestClassifier(MultinomialNB())\n",
    "MNB_classifier.fit(X_train_tfidf, y_train)\n",
    "y_pred_test=MNB_classifier.predict(X_test_tfidf)\n",
    "y_pred_train=MNB_classifier.predict(X_train_tfidf)\n",
    "y_pred_val=MNB_classifier.predict(X_val_tfidf)\n",
    "result_test['Multinomial_NB'] = y_pred_test\n",
    "result_val['Multinomial_NB'] = accuracy_score(y_pred_val,y_val)\n",
    "print (\"Accurary of Multinomial Naive Bayes Classifier on Training Data:\",accuracy_score(y_pred_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c50f9ae93273711cf55babfae3cec7100ba0de4a"
   },
   "outputs": [],
   "source": [
    "# Bernoulli Naive Bayes Model\n",
    "BNB_model = OneVsRestClassifier(BernoulliNB())\n",
    "BNB_model.fit(X_train_tfidf, y_train)\n",
    "y_pred_test=BNB_model.predict(X_test_tfidf)\n",
    "y_pred_train=BNB_model.predict(X_train_tfidf)\n",
    "y_pred_val=BNB_model.predict(X_val_tfidf)\n",
    "result_test['Bernoulli_NB'] = y_pred_test\n",
    "result_val['Bernoulli_NB'] = accuracy_score(y_pred_val,y_val)\n",
    "print('Accurary of Bernoulli Naive Bayes Classifier on Training Data:',accuracy_score(y_pred_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f193b1fa96ad3f24c34975c52599af222085dc7f"
   },
   "outputs": [],
   "source": [
    "#Ridge Classifier Model\n",
    "ridge_model = OneVsRestClassifier(RidgeClassifier(normalize=True))\n",
    "ridge_model.fit(X_train_tfidf, y_train)\n",
    "y_pred_test=ridge_model.predict(X_test_tfidf)\n",
    "y_pred_train=ridge_model.predict(X_train_tfidf)\n",
    "y_pred_val=ridge_model.predict(X_val_tfidf)\n",
    "result_test['Ridge_Classifier'] = y_pred_test\n",
    "result_val['Ridge_Classifier'] = accuracy_score(y_pred_val,y_val)\n",
    "print('Accurary of Ridge Classifier on Training Data:',accuracy_score(y_pred_train,y_train))                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c13c29fd4b04bca4ca1dc2df3229ff4a52416d5b"
   },
   "outputs": [],
   "source": [
    "# Logistic Regression Model\n",
    "log_model = OneVsRestClassifier(LogisticRegression(multi_class='ovr'))\n",
    "log_model.fit(X_train_tfidf, y_train)\n",
    "y_pred_test=log_model.predict(X_test_tfidf)\n",
    "y_pred_train=log_model.predict(X_train_tfidf)\n",
    "y_pred_val=log_model.predict(X_val_tfidf)\n",
    "result_test['Logistic_Regression'] = y_pred_test\n",
    "result_val['Logistic_Regression'] = accuracy_score(y_pred_val,y_val)\n",
    "print('Accurary of Logistic Regression on Training Data:',accuracy_score(y_pred_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "92e4d5fbe63c64f3ab668235f61611001a2d2111",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SVM Classifier Model\n",
    "svm_model = OneVsRestClassifier(LinearSVC(multi_class='ovr'))\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "y_pred_test=svm_model.predict(X_test_tfidf)\n",
    "y_pred_train=svm_model.predict(X_train_tfidf)\n",
    "y_pred_val=svm_model.predict(X_val_tfidf)\n",
    "result_test['SVM'] = y_pred_test\n",
    "result_val['SVM'] = accuracy_score(y_pred_val,y_val)\n",
    "print('Accurary of SVM Classifier on Training Data:',accuracy_score(y_pred_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "345bde1273d7b6aafc1febdf53a93abfd9077364"
   },
   "outputs": [],
   "source": [
    "#visualizations\n",
    "\n",
    "D=result_val\n",
    "plt.figure(figsize=(20, 7))\n",
    "plt.yticks( fontsize=20)\n",
    "plt.xticks(range(len(D)), list(D.keys()), fontsize=20)\n",
    "ax=plt.bar(range(len(D)), list(D.values()), align='center',width=0.8)\n",
    "plt.title(\"# Accuracy Score on Cross-Validation Set by different Models\\n\", fontsize=40)\n",
    "plt.ylabel('# Accuracy Range', fontsize=30)\n",
    "plt.xlabel('\\n#Model type ', fontsize=30)\n",
    "#adding the text labels\n",
    "for rect in ax:\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width()/2.0, height, '%f' % float(height), ha='center', va='bottom',fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1e879fa932bde1d33b907bcf34e6951f30e487f9"
   },
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning\n",
    "#grid_values = {'estimator__C': [0.3, 1.0, 30.0]}\n",
    "#svm_grid = GridSearchCV(svm_model, param_grid = grid_values, scoring = 'roc_auc')\n",
    "\n",
    "#print('Grid best parameter (max. accuracy): ', svm_grid.best_params_)\n",
    "#print('Grid best score (accuracy): ', svm_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da116bf9fa5a8072c4e8b27c8ac34bb35b6912c2"
   },
   "outputs": [],
   "source": [
    "# storing results of SVM Classifier as our result\n",
    "y_test = result_test['SVM']\n",
    "# combining final results with the original test data set\n",
    "output = pd.DataFrame(y_test, columns = train.columns[2:8], index = test.index)\n",
    "output = pd.concat([test, output], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "573d8fe90dcee0681832433a229315c51f55e791"
   },
   "outputs": [],
   "source": [
    "#Sample Submission\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "643eb96cc7080a042368ce6631ec43b7cc78a446"
   },
   "outputs": [],
   "source": [
    "# verifing data\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e972dbc6ca5f008c249559e4056160a8aee90ef0"
   },
   "outputs": [],
   "source": [
    "# verifing select random case, as per index from above code chunk\n",
    "output.comment_text[5902]\n",
    "output.iloc[5902,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "29acaac446163814c367f4a04a03047bc8a8b54b"
   },
   "outputs": [],
   "source": [
    "# quick summary for training, validation and test set respectively\n",
    "y_train.sum(axis=0)\n",
    "y_val.sum(axis=0)\n",
    "output.iloc[:,2:8].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb9e81ab67ea3bef39514eb9dd2b3d0e62038957"
   },
   "outputs": [],
   "source": [
    "#Final Submission\n",
    "my_submission = output.drop(['comment_text'], axis = 1, inplace = False)\n",
    "my_submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
